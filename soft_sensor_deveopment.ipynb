{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EskelinenElias/ADAML-project1/blob/main/soft_sensor_deveopment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0aa8be4",
      "metadata": {
        "id": "c0aa8be4"
      },
      "source": [
        "# Soft sensor development\n",
        "\n",
        "LUT University \\\n",
        "BM20A6100 Advanced Data Analysis and Machine Learning - Blended teaching, Lpr 1.9.2025-12.12.2025 \\\n",
        "1st Period - Project work - Intermediary Submission 2 - Data modeling plan\n",
        "\n",
        "Elias Eskelinen, Vili NiemelÃ¤ & Matti Aalto \\\n",
        "25.9.2025"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e078909",
      "metadata": {
        "id": "5e078909"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89258c90",
      "metadata": {
        "id": "89258c90"
      },
      "source": [
        "## Step 1. Initialization, data onboarding and exploration\n",
        "\n",
        "Initialize libraries, set random number state for reproducibility, and the directory where the figures should be saved to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7851a8da",
      "metadata": {
        "id": "7851a8da"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Set random state\n",
        "np.random.seed(42)\n",
        "\n",
        "# Set directory for figures\n",
        "figs_dir = \"figures\" if os.path.isdir(\"figures\") else \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nuVbIyLKWYsT",
      "metadata": {
        "id": "nuVbIyLKWYsT"
      },
      "source": [
        "Fetch data from kaggle:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "do5w1FHjWYOY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "do5w1FHjWYOY",
        "outputId": "4a35be48-1348-467b-86c6-0506f1ed1866"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Fetch the latest version of the dataset from kaggle\n",
        "data_dir = kagglehub.dataset_download(\"edumagalhaes/quality-prediction-in-a-mining-process\")\n",
        "data_path = os.path.join(data_dir, os.listdir(data_dir)[0]);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "U71MX11yVdPE",
      "metadata": {
        "id": "U71MX11yVdPE"
      },
      "source": [
        "Load data to memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "thHmbIefVZBc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thHmbIefVZBc",
        "outputId": "6a501b8c-01b3-4810-e68c-6adb4b36f3a8"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(data_path)\n",
        "print(data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "--2wz2EtVeoi",
      "metadata": {
        "id": "--2wz2EtVeoi"
      },
      "source": [
        "Format the data; date column as datetime objects, other columns as decimal numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6NHky7oJVf2t",
      "metadata": {
        "id": "6NHky7oJVf2t"
      },
      "outputs": [],
      "source": [
        "# Format date column as dates\n",
        "data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "# Format other columns as decimal numbers\n",
        "for c in data.columns[1:]: data[c] = data[c].str.replace(',', '.').astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90f278d0",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(data['date'], data['% Silica Feed'])\n",
        "plt.grid(which='both')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "041ca98d",
      "metadata": {},
      "source": [
        "Low and high frequency variables:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6af4f14e",
      "metadata": {},
      "outputs": [],
      "source": [
        "low_freq_variables = ['% Iron Feed', '% Silica Feed', '% Iron Concentrate', '% Silica Concentrate']\n",
        "low_freq_features = ['% Iron Feed', '% Silica Feed']\n",
        "high_freq_features = [c for c in data.columns if c not in [*low_freq_variables, 'date']]\n",
        "predicted_variables = ['% Silica Concentrate']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a34619b9",
      "metadata": {
        "id": "a34619b9"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-y_40xTraat_",
      "metadata": {
        "id": "-y_40xTraat_"
      },
      "source": [
        "## Step 2. Data pretreatment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef28814e",
      "metadata": {},
      "source": [
        "We handle missing data by filling it in with the next sample, as the number of missing samples is neglible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02d20c4b",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(data.shape)\n",
        "# Find datetime stamps with less than 180 entries\n",
        "for date in data['date'].unique(): \n",
        "\n",
        "    # Count current datetime entries\n",
        "    num_entries = len(data[data['date'] == date])\n",
        "    if (num_missing_rows := 180 - num_entries) > 0: \n",
        "        \n",
        "        # Add missing entries to the start of the current datetime run\n",
        "        first_index = data['date'][data['date'] == date].idxmin()\n",
        "        missing_rows = pd.DataFrame([data.iloc[first_index, :].to_numpy()] * num_missing_rows, columns=data.columns)\n",
        "\n",
        "        # Add copies of the first instance of the datetime \n",
        "        data = pd.concat([\n",
        "            data[data['date'] < date], \n",
        "            missing_rows, \n",
        "            data[data['date'] >= date], \n",
        "        ], ignore_index = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47d3b04c",
      "metadata": {},
      "source": [
        "### Fixing the datetime index\n",
        "\n",
        "The data datetime index is missing minute and second values. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65cffb61",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create proper datetime index spaced by 20 seconds\n",
        "data['date_hour'] = pd.to_datetime(data['date'], format='%Y-%m-%d %H')\n",
        "data['within_hour_idx'] = data.groupby('date_hour').cumcount()\n",
        "data['datetime'] = data['date_hour'] + pd.to_timedelta(data['within_hour_idx'] * 20, unit='s')\n",
        "\n",
        "# Set the datetime as data index\n",
        "data = data.set_index('datetime')\n",
        "data.index.name = None\n",
        "data = data.sort_index()\n",
        "data = data.drop(columns=['date', 'within_hour_idx', 'date_hour'])\n",
        "\n",
        "# Resample the data \n",
        "print(data[data.columns[:3]].head())\n",
        "print(data[data.columns[:3]].tail())\n",
        "\n",
        "print(data.shape)\n",
        "print(data.dropna().shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9d138a1",
      "metadata": {},
      "source": [
        "### Resampling the data\n",
        "\n",
        "Resample the data to the frequency `sampling_freq`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0474bc1d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Decide sampling frequency and resample the data\n",
        "sampling_freq = '1H' # For example: 1H, 5MIN, 20S\n",
        "data = data.resample(sampling_freq).mean()\n",
        "\n",
        "# Resample the data \n",
        "print(data[data.columns[:6]].head())\n",
        "print(data[data.columns[:3]].tail())\n",
        "\n",
        "print(data.shape)\n",
        "data = data.dropna()\n",
        "print(data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b148af33",
      "metadata": {},
      "source": [
        "We start by dropping bad time windows from the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0238147e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime as dt\n",
        "# Usable time windows in the data\n",
        "\n",
        "dt_format = '%Y-%m-%d %H:%M:%S'\n",
        "good_windows = [\n",
        "    {'start': dt.strptime(\"2017-03-31 01:00:00\", dt_format), 'end': dt.strptime(\"2017-05-13 23:59:59\", dt_format)}, \n",
        "    {'start': dt.strptime(\"2017-06-15 01:00:00\", dt_format), 'end': dt.strptime(\"2017-07-24 23:59:59\", dt_format)}, \n",
        "    {'start': dt.strptime(\"2017-08-16 01:00:00\", dt_format), 'end': dt.strptime(\"2017-10-01 23:59:59\", dt_format)}\n",
        "]\n",
        "\n",
        "# Reconstruct the dataset from the usable sets\n",
        "data_batches = [data.loc[(data.index > w['start']) & (data.index < w['end'])] for w in good_windows]\n",
        "#data = pd.concat(data_batches)\n",
        "print(data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7791491",
      "metadata": {
        "id": "e7791491"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf97364d",
      "metadata": {
        "id": "cf97364d"
      },
      "source": [
        "## Step 3. Data visualization and PCA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cc2767c",
      "metadata": {
        "id": "2cc2767c"
      },
      "source": [
        "### Fitting the model\n",
        "\n",
        "Fitting the PCA model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d257f62",
      "metadata": {
        "id": "1d257f62"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Normalize the data for PCA\n",
        "scaler = StandardScaler()\n",
        "PCA_data = pd.DataFrame(scaler.fit_transform(data))\n",
        "\n",
        "# Fit the PCA model\n",
        "pca = PCA().fit(PCA_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "701d7306",
      "metadata": {
        "id": "701d7306"
      },
      "source": [
        "### Explained variance\n",
        "\n",
        "Visualizing the the principal components values and cumulative explained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c7ce608",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "7c7ce608",
        "outputId": "ead8ef2c-3315-4e0b-a64d-b9994b27ab3c"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "# Principal component values\n",
        "axs[0].plot(pca.singular_values_)\n",
        "axs[0].set_title(\"Principal component values\")\n",
        "axs[0].set_xlabel(\"Principal component\")\n",
        "axs[0].set_ylabel(\"Principal component value\")\n",
        "axs[0].grid()\n",
        "\n",
        "# Cumulative explained variance\n",
        "axs[1].plot(pca.explained_variance_ratio_.cumsum())\n",
        "axs[1].set_title(\"Cumulative explained variance\")\n",
        "axs[1].set_xlabel(\"Principal component\")\n",
        "axs[1].set_ylabel(\"Cumulative explained variance\")\n",
        "axs[1].grid()\n",
        "\n",
        "plt.tight_layout()\n",
        "if figs_dir: plt.savefig(os.path.join(figs_dir, \"pca_explained_variance.png\"), dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "600c73c2",
      "metadata": {
        "id": "600c73c2"
      },
      "source": [
        "### PCA biplots\n",
        "\n",
        "Next we create biplots for the PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c959c6c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4c959c6c",
        "outputId": "f68e0a11-8269-4dd6-d996-eb4c75ac0449"
      },
      "outputs": [],
      "source": [
        "from itertools import combinations\n",
        "\n",
        "# Get scores and loadings\n",
        "scores = pca.transform(PCA_data)\n",
        "loadings = pca.components_.T\n",
        "\n",
        "# Scale loadings to match the scale of scores for better visualization\n",
        "scaled_loadings = loadings * np.max(np.abs(scores))\n",
        "\n",
        "# Choose the number of principal components to plot\n",
        "num_components = 3\n",
        "\n",
        "# Plots for each combination of principal components\n",
        "for (i, j) in combinations(range(min(num_components, loadings.shape[1])), 2):\n",
        "\n",
        "    text_locations = []\n",
        "\n",
        "    # Plot datapoints\n",
        "    plt.scatter(scores[:, i], scores[:, j], marker='.', alpha=0.5, label=\"Samples\")\n",
        "\n",
        "    # Plot arrows for loadings\n",
        "    for k, feature in enumerate(PCA_data.columns):\n",
        "        u, v = scaled_loadings[k, i], scaled_loadings[k, j]\n",
        "        plt.arrow(0, 0, u, v, color='r', alpha=0.7, head_width=0.05)\n",
        "        plt.text(1.05*u, 1.05*v, feature, color='r', fontsize=6)\n",
        "\n",
        "    # Add reference lines\n",
        "    plt.axhline(0, color='grey', linewidth=1)\n",
        "    plt.axvline(0, color='grey', linewidth=1)\n",
        "\n",
        "    # Labels\n",
        "    plt.xlabel(f\"PC{i+1}\")\n",
        "    plt.ylabel(f\"PC{j+1}\")\n",
        "    plt.title(f\"PCA Biplot (PC{i+1}, PC{j+1})\")\n",
        "    #plt.legend()\n",
        "    plt.grid()\n",
        "    if figs_dir: plt.savefig(os.path.join(figs_dir, f\"PCA_biplot_PC{i+1}_PC{j+1}.png\"), dpi=300)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10a7bff5",
      "metadata": {
        "id": "10a7bff5"
      },
      "source": [
        "### Data correlations and distributions\n",
        "\n",
        "We visualize data correlations and variable distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8qVwqNHwicEc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8qVwqNHwicEc",
        "outputId": "93adb8ac-11af-41c5-98b8-8b57c6a7580a"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# Correlation matrices\n",
        "data_corr = data.iloc[:,1:].corr()\n",
        "plt.figure(figsize=(18, 15))\n",
        "sns.heatmap(data_corr, annot=True, cmap='coolwarm', fmt=\".2f\", square=True)\n",
        "if figs_dir: plt.savefig(os.path.join(figs_dir, f\"correlation_matrix.png\"), dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Histograms\n",
        "data.hist(figsize=(20, 15))\n",
        "plt.tight_layout()\n",
        "if figs_dir: plt.savefig(os.path.join(figs_dir, f\"variable_distributions.png\"), dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0bce2a3",
      "metadata": {
        "id": "e0bce2a3"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55d5466d",
      "metadata": {
        "id": "55d5466d"
      },
      "source": [
        "## Step 4. Model calibration\n",
        "\n",
        "### Preparations\n",
        "\n",
        "Our data modeling process begins from splitting the data to training, validation and test sets. Then we train the dynamic model on\n",
        "\n",
        "\n",
        "We start by defining the predicted variable and the predictor variables:\n",
        "\n",
        "**NOTE!** '% Iron Concentrate' is dropped as it can't be used (A-level task)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4befa7e3",
      "metadata": {
        "id": "4befa7e3"
      },
      "outputs": [],
      "source": [
        "TARGET_COLUMN = '% Silica Concentrate'\n",
        "DROP_COLUMNS = ['date']\n",
        "\n",
        "# Select feature columns\n",
        "feature_columns = [c for c in data.columns if c not in [*DROP_COLUMNS, TARGET_COLUMN]]\n",
        "print(feature_columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04c11ab0",
      "metadata": {
        "id": "04c11ab0"
      },
      "source": [
        "We split the data to training set, validation set and test set. The time series data is divided into 3 distinct windows (we dropped the unusable windows in Data Pretreatment), which gives us a natural way to split the data to the 3 sets. The start and end dates of these time windows are stored in the `start_dates` and `end_dates` arrays, which we can use to split the dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41d9f97d",
      "metadata": {
        "id": "41d9f97d"
      },
      "outputs": [],
      "source": [
        "# Extract usable time windows from the data\n",
        "data1, data2, data3 = [data.loc[(data.index > w['start']) & (data.index < w['end'])] for w in good_windows]\n",
        "print(data1.shape, data2.shape, data3.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "218a0e7b",
      "metadata": {
        "id": "218a0e7b"
      },
      "source": [
        "We split the data to sets of $X$ and $y$ and scale the data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "362db456",
      "metadata": {
        "id": "362db456"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Split the training, validation and test sets to sets X and y\n",
        "X_train1, y_train1 = data1[feature_columns], data1[TARGET_COLUMN]\n",
        "X_train2, y_train2 = data2[feature_columns], data2[TARGET_COLUMN]\n",
        "X_test, y_test = data3[feature_columns], data3[TARGET_COLUMN]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1e22ff6",
      "metadata": {
        "id": "a1e22ff6"
      },
      "source": [
        "#### Adding lagged variables\n",
        "\n",
        "We implement a function `make_lagged` for adding lagged variables as new columns to dataset `X`. If both `X` and `y` are passed, `y` the top rows are cut so that `X` and `y` are the same length. The added lags can be changed by passing the array `lags` and the columns which lagged variables are added for can be changed by passing `columns`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbf95b34",
      "metadata": {
        "id": "cbf95b34"
      },
      "outputs": [],
      "source": [
        "def make_lagged(X: np.ndarray, y: np.ndarray|None=None, lags: np.ndarray=None, columns: list|None=None) -> np.ndarray|tuple[np.ndarray, np.ndarray]:\n",
        "    \n",
        "    # By default return unlagged features and features with lag 1\n",
        "    if lags is None: lags = np.array([0, 1])\n",
        "    \n",
        "    # By default add lagged variables for all columns\n",
        "    if columns is None: columns = X.columns \n",
        "\n",
        "    # Create lagged variables and add them to X\n",
        "    lagged_vars = [X] if min(lags) < 1 else []\n",
        "    lagged_vars += [X[columns].shift(lag).add_suffix(f\"_lag{lag}\" if lag > 0 else \"\") for lag in lags if lag > 0]\n",
        "    X_lagged = pd.concat(lagged_vars, axis=1).dropna(axis=0)\n",
        "\n",
        "    # Return X, or if y was passed, X and y cut to the same length as X (from the top)\n",
        "    if y is None: return X_lagged\n",
        "    return X_lagged, y[max(lags):]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8803b24",
      "metadata": {
        "id": "c8803b24"
      },
      "source": [
        "### Selecting the optimal number of latent variables and lags\n",
        "\n",
        "We select the optimal number of lags by training the model with different number of lags and cross-validating the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33ca037a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "33ca037a",
        "outputId": "9e9d5528-c5ff-4144-8dd4-ea30fcbe4db0"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Training parameters\n",
        "max_lv = len(feature_columns)\n",
        "min_lag = 0 # The minimum amount of lag; a value greater than 0 means the original variables are emitted\n",
        "max_lag = 10 # The highest amount of lag \n",
        "selected_lags_range = [np.arange(min_lag, i) for i in range(min_lag + 1, max_lag + 1)]\n",
        "num_components_range = np.arange(1, max_lv)\n",
        "lagged_columns = X_train1.columns\n",
        "\n",
        "# Result arrays\n",
        "results = []\n",
        "\n",
        "# Training data split and scoring\n",
        "root_mean_square_error = lambda y_true, y_pred : np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "# Cross validation datasets\n",
        "cv_sets = [\n",
        "    (X_train1, y_train1, X_train2, y_train2), \n",
        "    (X_train2, y_train2, X_train1, y_train1), \n",
        "    # (X_train1, y_train1, X_test, y_test), \n",
        "    # (X_test, y_test, X_train1, y_train1), \n",
        "    # (X_train2, y_train2, X_test, y_test), \n",
        "    # (X_test, y_test, X_train2, y_train2), \n",
        "]\n",
        "\n",
        "# Loop over lags\n",
        "for i, selected_lags in enumerate(selected_lags_range):\n",
        "\n",
        "    # Make lagged cross-validation sets\n",
        "    cv_sets_lagged = []\n",
        "    for X_train, y_train, X_val, y_val in cv_sets: \n",
        "\n",
        "        # Add lagged variables to training and validation sets\n",
        "        X_train_lagged, y_train_lagged = make_lagged(X_train, y_train, selected_lags, lagged_columns)\n",
        "        X_val_lagged, y_val_lagged = make_lagged(X_val, y_val, selected_lags, lagged_columns)\n",
        "\n",
        "        #################################################################################################################\n",
        "        # IMPORTANT! drop the UNLAGGED '% Iron Concentrate' column\n",
        "        X_train_lagged, X_val_lagged = X_train_lagged.drop(columns=['% Iron Concentrate']), X_val_lagged.drop(columns=['% Iron Concentrate'])\n",
        "        #################################################################################################################\n",
        "\n",
        "        # Add lagged sets to cross-validation sets array\n",
        "        cv_sets_lagged.append((X_train_lagged, y_train_lagged, X_val_lagged, y_val_lagged))\n",
        "\n",
        "    # Cross validate the model over different number of latent variables\n",
        "    for j, num_components in enumerate(num_components_range):\n",
        "        \n",
        "        # Define model \n",
        "        scaler = StandardScaler()\n",
        "        model = PLSRegression(num_components)\n",
        "\n",
        "        # Prepare results arrays\n",
        "        y_test_cv, y_pred_cv = [], []\n",
        "\n",
        "        # Cross-validate model \n",
        "        for X_train, y_train, X_val, y_val in cv_sets_lagged: \n",
        "            \n",
        "            # Scale the training and validation data\n",
        "            X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
        "            X_val = pd.DataFrame(scaler.transform(X_val))\n",
        "\n",
        "            # Fit the model \n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "            # Predict the test set\n",
        "            y_pred = model.predict(X_val)\n",
        "            y_test_cv.append(y_val.to_numpy().reshape(-1, 1))\n",
        "            y_pred_cv.append(y_pred.reshape(-1, 1))\n",
        "\n",
        "        # Compute stats\n",
        "        y_test_cv, y_pred_cv = np.concat(y_test_cv, axis=0), np.concat(y_pred_cv,  axis=0)\n",
        "        PRESS = np.sum((y_test_cv - y_pred_cv) ** 2)\n",
        "        TSS = np.sum((y_test_cv - np.mean(y_test_cv)) ** 2)\n",
        "        Q2 = 1 - PRESS / TSS if TSS != 0 else np.nan\n",
        "        RMSE = root_mean_square_error(y_test_cv, y_pred_cv)\n",
        "        \n",
        "        # Store results\n",
        "        results.append({\n",
        "            \"num_comps\": num_components, \n",
        "            \"num_lags\": selected_lags[-1], \n",
        "            \"PRESS\": PRESS, \n",
        "            \"Q2\": Q2,\n",
        "            \"RMSE\": RMSE\n",
        "        })\n",
        "\n",
        "results = pd.DataFrame(results)\n",
        "print(results.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ae2f1e6",
      "metadata": {},
      "source": [
        "### Visualize cross-validation results\n",
        "\n",
        "We visualize cross-validation results with PRESS, Q2 and RMSE plots to make model selection easier. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a095a2f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# PRESS scores for different lags over number of latent variables\n",
        "for num_lags, grouped_results in results.groupby('num_lags'):\n",
        "    plt.plot(grouped_results['num_comps'], grouped_results['PRESS'], label=f'Lag {num_lags}')\n",
        "plt.title(\"PRESS scores over number of LVs\")\n",
        "plt.xlabel(\"Number of latent variables\")\n",
        "plt.ylabel(\"PRESS score\")\n",
        "plt.legend(loc='upper right')\n",
        "plt.grid()\n",
        "if figs_dir: plt.savefig(os.path.join(figs_dir, f\"PRESS_cross_validation.png\"), dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Q2 scores for different lags over number of latent variables\n",
        "for num_lags, grouped_results in results.groupby('num_lags'):\n",
        "    plt.plot(grouped_results['num_comps'], grouped_results['Q2'], label=f'Lag {num_lags}')\n",
        "plt.title(\"Q2 scores over number of LVs\")\n",
        "plt.xlabel(\"Number of latent variables\")\n",
        "plt.ylabel(\"Q2 score\")\n",
        "plt.legend(loc='upper right')\n",
        "plt.grid()\n",
        "if figs_dir: plt.savefig(os.path.join(figs_dir, f\"Q2_cross_validation.png\"), dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# RMSE scores for different lags over number of latent variables\n",
        "for num_lags, grouped_results in results.groupby('num_lags'):\n",
        "    plt.plot(grouped_results['num_comps'], grouped_results['RMSE'], label=f'Lag {num_lags}')\n",
        "plt.title(\"RMSE scores over number of LVs\")\n",
        "plt.xlabel(\"Number of latent variables\")\n",
        "plt.ylabel(\"RMSE score\")\n",
        "plt.legend(loc='upper right')\n",
        "plt.grid()\n",
        "if figs_dir: plt.savefig(os.path.join(figs_dir, f\"RMSE_cross_validation.png\"), dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# PRESS scores as a heatmap\n",
        "pivot_press = results.pivot(index='num_lags', columns='num_comps', values='PRESS')\n",
        "sns.heatmap(pivot_press.astype(float), cmap=\"crest\", \n",
        "    xticklabels=num_components_range,\n",
        "    yticklabels=[selected_lags[-1] for selected_lags in selected_lags_range],\n",
        ")\n",
        "plt.xlabel('Number of latent variables')\n",
        "plt.ylabel('Amount of lag')\n",
        "plt.title(f\"PRESS scores as a heatmap\")\n",
        "if figs_dir: plt.savefig(os.path.join(figs_dir, f\"PRESS_heatmap.png\"), dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f16ca0d7",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02f3f2a3",
      "metadata": {},
      "source": [
        "## Step 5. Model validation\n",
        "\n",
        "We choose calibrated model parameters and cross validate model performance using k-folds cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3f18ad0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Choose best model parameters according to PRESS\n",
        "i = results['PRESS'].argmin()\n",
        "j = results['Q2'].argmax()\n",
        "print(f\"Best by PRESS: model {i}, best by Q2: model {j}\")\n",
        "row = results.iloc[i] \n",
        "print(row)\n",
        "\n",
        "# Best model parameters\n",
        "num_components, max_lag = row['num_comps'].astype(int), row['num_lags']\n",
        "selected_lags = np.arange(0, max_lag + 1).astype(int)\n",
        "selected_cols = feature_columns\n",
        "\n",
        "# Create lagged datasets\n",
        "X_train_lagged, y_train_lagged = make_lagged(X_train1, y_train1, selected_lags, selected_cols)\n",
        "X_val_lagged, y_val_lagged = make_lagged(X_train2, y_train2, selected_lags, selected_cols)\n",
        "\n",
        "#################################################################################################################\n",
        "# IMPORTANT! drop the UNLAGGED '% Iron Concentrate' column\n",
        "X_train_lagged, X_val_lagged = X_train_lagged.drop(columns=['% Iron Concentrate']), X_val_lagged.drop(columns=['% Iron Concentrate'])\n",
        "#################################################################################################################\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_lagged = pd.DataFrame(scaler.fit_transform(X_train_lagged), columns=X_train_lagged.columns)\n",
        "X_val_lagged = pd.DataFrame(scaler.transform(X_val_lagged),  columns=X_train_lagged.columns)\n",
        "\n",
        "# Train model with optimal parameters\n",
        "model = PLSRegression(num_components)\n",
        "model.fit(X_train_lagged, y_train_lagged)\n",
        "print(model.coef_)\n",
        "\n",
        "# Predict the validation set\n",
        "y_pred = model.predict(X_val_lagged)\n",
        "\n",
        "# Compute stats\n",
        "PRESS = np.sum((y_val_lagged - y_pred) ** 2)\n",
        "TSS = np.sum((y_val_lagged - np.mean(y_val_lagged)) ** 2)\n",
        "Q2 = 1 - PRESS / TSS if TSS != 0 else np.nan\n",
        "RMSE = root_mean_square_error(y_val_lagged, y_pred)\n",
        "print(f\"PRESS: {PRESS:.4f}\")\n",
        "print(f\"Q2: {Q2:.4f}\")\n",
        "\n",
        "# Plot the prediction\n",
        "plt.figure()\n",
        "plt.plot(y_val_lagged.index, y_val_lagged, label='target')\n",
        "plt.plot(y_val_lagged.index, y_pred, label='predicted')\n",
        "plt.legend()\n",
        "plt.title(\"Model prediction on validation set\")\n",
        "plt.ylabel(\"% Silica Concentrate\")\n",
        "plt.xlabel(\"Datetime\")\n",
        "if figs_dir: plt.savefig(os.path.join(figs_dir, f\"best_model_val_pred.png\"), dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Plot model coefficients\n",
        "plt.bar(X_train_lagged.columns.ravel(), model.coef_.flatten())\n",
        "plt.title(\"Model prediction on validation set\")\n",
        "plt.ylabel(\"% Silica Concentrate\")\n",
        "plt.xlabel(\"Datetime\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.grid()\n",
        "if figs_dir: plt.savefig(os.path.join(figs_dir, f\"best_model_coefs.png\"), dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6a4e788",
      "metadata": {},
      "source": [
        "### Reducing model vars\n",
        "\n",
        "We reduce model variables by choosing the variables with the highest absolute coefficients, fit the reduced model and visualize performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "500e4bbb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find the most important model variables\n",
        "treshold = 0.05\n",
        "index = np.abs(model.coef_.flatten()) > treshold\n",
        "columns = X_train_lagged.columns[index]\n",
        "print(columns)\n",
        "\n",
        "# Create reduced training and validation datasets\n",
        "X_train_reduced, X_val_reduced = X_train_lagged[columns], X_val_lagged[columns]\n",
        "\n",
        "# Fit the reduced model \n",
        "reduced_model = PLSRegression(num_components)\n",
        "reduced_model.fit(X_train_reduced, y_train_lagged)\n",
        "print(reduced_model.coef_)\n",
        "\n",
        "# Predict the validation set\n",
        "y_pred = reduced_model.predict(X_val_reduced)\n",
        "\n",
        "# Compute stats\n",
        "PRESS = np.sum((y_val_lagged - y_pred) ** 2)\n",
        "TSS = np.sum((y_val_lagged - np.mean(y_val_lagged)) ** 2)\n",
        "Q2 = 1 - PRESS / TSS if TSS != 0 else np.nan\n",
        "RMSE = root_mean_square_error(y_val_lagged, y_pred)\n",
        "print(f\"PRESS: {PRESS:.4f}\")\n",
        "print(f\"Q2: {Q2:.4f}\")\n",
        "\n",
        "# Plot the prediction\n",
        "plt.figure()\n",
        "plt.plot(y_val_lagged.index, y_val_lagged, label='target')\n",
        "plt.plot(y_val_lagged.index, y_pred, label='predicted')\n",
        "plt.legend()\n",
        "plt.title(\"Model prediction on validation set\")\n",
        "plt.ylabel(\"% Silica Concentrate\")\n",
        "plt.xlabel(\"Datetime\")\n",
        "if figs_dir: plt.savefig(os.path.join(figs_dir, f\"best_model_reduced_pred.png\"), dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Plot model coefficients\n",
        "plt.bar(columns, reduced_model.coef_.flatten())\n",
        "plt.title(\"Model prediction on validation set\")\n",
        "plt.ylabel(\"% Silica Concentrate\")\n",
        "plt.xlabel(\"Datetime\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.grid()\n",
        "if figs_dir: plt.savefig(os.path.join(figs_dir, f\"best_model_reduced_coefs.png\"), dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0983b2a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "validated_model = reduced_model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edd7ec5a",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ce53d87",
      "metadata": {},
      "source": [
        "## Step 6. Model testing \n",
        "\n",
        "Apply validated model to the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f327dde4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a lagged, reduced version of the test set\n",
        "X_test_lagged, y_test_lagged = make_lagged(X_test, y_test, selected_lags, selected_cols)\n",
        "\n",
        "#################################################################################################################\n",
        "# IMPORTANT! drop the UNLAGGED '% Iron Concentrate' column\n",
        "X_test_lagged = X_test_lagged.drop(columns=['% Iron Concentrate'])\n",
        "#################################################################################################################\n",
        "\n",
        "# Create a reduced test set \n",
        "X_test_reduced = X_test_lagged[columns]\n",
        "\n",
        "# Scale the input data\n",
        "X_test_reduced = pd.DataFrame(scaler.fit_transform(X_test_reduced), columns=columns)\n",
        "\n",
        "# Predict the test set using the best model \n",
        "y_pred_test = validated_model.predict(X_test_reduced)\n",
        "\n",
        "# Compute stats\n",
        "PRESS = np.sum((y_val_lagged - y_pred) ** 2)\n",
        "TSS = np.sum((y_val_lagged - np.mean(y_val_lagged)) ** 2)\n",
        "Q2 = 1 - PRESS / TSS if TSS != 0 else np.nan\n",
        "RMSE = root_mean_square_error(y_val_lagged, y_pred)\n",
        "print(f\"PRESS: {PRESS:.4f}\")\n",
        "print(f\"Q2: {Q2:.4f}\")\n",
        "\n",
        "# Plot the test set\n",
        "plt.figure()\n",
        "plt.plot(X_test_reduced.index, y_test_lagged, label='target')\n",
        "plt.plot(X_test_reduced.index, y_pred_test, label='predicted')\n",
        "plt.legend()\n",
        "plt.title(\"Model prediction on test set\")\n",
        "plt.ylabel(\"% Silica Concentrate\")\n",
        "plt.xlabel(\"Datetime\")\n",
        "if figs_dir: plt.savefig(os.path.join(figs_dir, f\"best_model_test_pred.png\"), dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73e089b9",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be5e364b",
      "metadata": {},
      "source": [
        "## Dynamic model implementation\n",
        "\n",
        "The dynamic PLS model predicts $y_t$ from $\\mathbf{x}_t$ by fitting a PLS regression at each $t$ to $\\mathbf{X}_{t, w}=(\\mathbf{x}_{t-1}, ..., \\mathbf{x}_{t-1-w})$ and $\\mathbf{y}_{t, w}=(y_{t-1}, ..., y_{t-1-w})$, where $t$ is time at $y_t$, $\\mathbf{x_t}$ and $w$ is the size of the window of previous observations that are considered. \n",
        "\n",
        "We implement the dynamic PLS model `DynamicPLSModel`. The model is initialized with `num_components` i.e. the number of latent variables, and `window_size` i.e. the number of previous observations that are considered when predicting $y_t$ at each $t$. There are four class methods: \n",
        "\n",
        "- `fit`: this is used to fit the model to `X_train` and `y_train` i.e. $\\mathbf{X}_{t, w}=(\\mathbf{x}_{t-1}, ..., \\mathbf{x}_{t-1-w})$ at $t$,\n",
        "\n",
        "- `predict`: this is used to predict $y_t$ from $\\mathbf{x}_t$ (`X`) after running `fit`, \n",
        "\n",
        "- `fit_predict`: a combination of `fit` and `predict` which fits the model to $\\mathbf{X}_{t, w}=(\\mathbf{x}_{t-1}, ..., \\mathbf{x}_{t-1-w})$ (`X_train` and `y_train`) and then predicts $y_t$ from $\\mathbf{x}_t$ (`X`). For simplicity and clarity, this method should be used instead of `fit` and `predict` separately, when predicting $y_t$ at a single $t$. \n",
        "\n",
        "- `rolling_predict`: this can be used for calibration, validation and testing purposes. Predicts $y_t$ at each $\\mathbf{x}_t$ by fitting the model to each window $\\mathbf{X}_{t, w}=(\\mathbf{x}_{t-1}, ..., \\mathbf{x}_{t-1-w})$ and $\\mathbf{y}_{t, w}=(y_{t-1}, ..., y_{t-1-w})$ in `X`, `y`. Returns the predictions `y_pred` and a set of corresponding true labels `y_true`. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02f9e4eb",
      "metadata": {},
      "outputs": [],
      "source": [
        "class DynamicPLSModel: \n",
        "    def __init__(self, num_components: int, window_size: int): \n",
        "        self.window_size = window_size\n",
        "        self.num_components = num_components\n",
        "        self.model = None\n",
        "\n",
        "    # Fit model to X_{t-1},...,X_{t-w-1}, y_{t-1},...,y_{t-w-1}\n",
        "    def fit(self, X_train: np.ndarray, y_train: np.ndarray) -> np.ndarray: \n",
        "        assert X_train.ndim == 2 and y_train.ndim in [1, 2], 'X_train must be a matrix and y_train must be a matrix or a vector'\n",
        "        assert len(X_train) == len(y_train), 'X_train and y_train must be the same length'\n",
        "        X_train, Y_train = np.array(X_train), np.array(y_train).reshape(len(y_train), -1)\n",
        "        self.X_scaler, self.Y_scaler = StandardScaler().fit(X_train), StandardScaler().fit(Y_train)\n",
        "        X_train, Y_train = self.X_scaler.transform(X_train), self.Y_scaler.transform(Y_train)\n",
        "        self.model = PLSRegression(n_components=self.num_components).fit(X_train, Y_train)\n",
        "        return self\n",
        "    \n",
        "    # Predict y_t from X_t (assuming model was fit to X_{t-1},...,X_{t-w-1}, y_{t-1},...,y_{t-w-1})\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray: \n",
        "        assert self.model is not None, 'Model has not been trained'\n",
        "        assert len(X) == 1 and X.ndim == 2, \"X must be a matrix of length 1\"\n",
        "        Y_pred = self.model.predict(self.X_scaler.transform(np.array(X)))\n",
        "        return self.Y_scaler.inverse_transform(Y_pred).flatten()\n",
        "    \n",
        "    # Fit model to X_{t-1},...,X_{t-w-1}, y_{t-1},...,y_{t-w-1}, predict y_t from X_t\n",
        "    def fit_predict(self, X: np.ndarray, X_train: np.ndarray, y_train: np.ndarray) -> np.ndarray: \n",
        "        return self.fit(X_train, y_train).predict(X)\n",
        "    \n",
        "    # Rolling window predict: predict y_pred_t for each X_t by fitting model to X_{t-1},...,X_{t-w-1}, y_{t-1},...,y_{t-w-1}\n",
        "    def rolling_predict(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
        "        assert X.ndim == 2 and y.ndim in [1, 2], 'X must be a matrix and y must be a matrix or a vector'\n",
        "        assert len(X) == len(y) > 2, 'X and y must be the same length that is greater than 2'\n",
        "        X_train, y_train = np.array(X), np.array(y)\n",
        "        y_pred = np.zeros(max(len(X_train) - self.window_size, 1)) # Prepare an array for the predictions\n",
        "        for i, t in enumerate(range(min(self.window_size + 1, len(X_train)-1), len(X_train))): \n",
        "            self.fit(X_train[i:t], y_train[i:t]) # Fit the model to the window\n",
        "            y_pred[i] = self.predict(X_train[t].reshape(1, -1)) # Predict the next value \n",
        "        y_true = y[self.window_size:] if self.window_size < len(X_train) else np.array([y[-1]]) # Cut true labels without a prediction (oldest ones)\n",
        "        return y_pred, y_true # Return predictions and true labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77707fa0",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7530206",
      "metadata": {},
      "source": [
        "## Model calibration for dynamic model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c93fdbe",
      "metadata": {},
      "source": [
        "We prepare functions for evaluating model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "563d7efa",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Function to compute RMSE score\n",
        "eval_RMSE = lambda y_true, y_pred : np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "# Function to compute PRESS score\n",
        "eval_PRESS = lambda y_true, y_pred: np.sum((y_true - y_pred) ** 2)\n",
        "\n",
        "# Function to compute Q2 score\n",
        "def eval_Q2(y_true: np.ndarray, y_pred: np.ndarray) -> float: \n",
        "    TSS = np.sum((y_true - np.mean(y_true)) ** 2)\n",
        "    return 1 - eval_PRESS(y_true, y_pred) / TSS if TSS != 0 else np.nan"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8133c720",
      "metadata": {},
      "source": [
        "We evaluate model performance with different combinations of the number of lags, number of latent variables and window size. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a42156a3",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Selected lags options to search over\n",
        "max_lag_range = np.arange(0, 11)\n",
        "num_comps_range = np.arange(1, 11)\n",
        "window_size_range = np.power(2, np.arange(5, 10)) \n",
        "\n",
        "# Prepare a list of model configurations\n",
        "configs = [(ml, nc, ws) for ml in max_lag_range for nc in num_comps_range for ws in window_size_range]\n",
        "\n",
        "# Prepare an array for results\n",
        "results = []\n",
        "\n",
        "# Loop over lags\n",
        "print(f\"Evaluating {len(configs)} model configurations\")\n",
        "for i, (max_lag, num_comps, window_size) in enumerate(tqdm(configs)):\n",
        "\n",
        "    # Make lagged dataset\n",
        "    X_train_lags, y_train_lags = make_lagged(X_train, y_train, np.arange(0, max_lag + 1))\n",
        "    \n",
        "    # Drop unwanted features\n",
        "    X_train_lags = X_train_lags.drop(columns=['% Iron Concentrate'])\n",
        "\n",
        "    # Initialize model \n",
        "    model = DynamicPLSModel(num_comps, window_size)\n",
        "\n",
        "    # Predict the calibration set using rolling prediction\n",
        "    y_pred, y_true = model.rolling_predict(X_train_lags, y_train_lags)\n",
        "\n",
        "    # Evaluate performance\n",
        "    results.append({\n",
        "        \"number of components\": num_comps, \n",
        "        \"number of lags\": max_lag, \n",
        "        \"window size\": window_size,\n",
        "        \"PRESS\": eval_PRESS(y_true, y_pred), \n",
        "        \"Q2\": eval_Q2(y_true, y_pred),\n",
        "        \"RMSE\": eval_RMSE(y_true, y_pred)\n",
        "    })\n",
        "\n",
        "# Process results\n",
        "results = pd.DataFrame(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56007de8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate summary statistics: models grouped by maximum lag and window size, min PRESS scores and max Q2 scores achieved\n",
        "summary = results.groupby(['number of lags', 'window size']).agg(PRESS=('PRESS', 'min'), Q2=('Q2', 'max'))\n",
        "\n",
        "# PRESS over number of components for 10 models which achieved the lowest PRESS score \n",
        "plt.figure()\n",
        "for max_lag, win_size in summary.nsmallest(10, columns='PRESS').index: \n",
        "    model_results = results[(results['number of lags']==max_lag) & (results['window size']==win_size)]\n",
        "    model_results = model_results.sort_values(\"number of components\")\n",
        "    plt.plot(model_results['number of components'], model_results['PRESS'], label=f\"lags {max_lag}, wsize {win_size}\")\n",
        "plt.grid()\n",
        "plt.title(\"PRESS over number of latent variables; 10 best models\")\n",
        "plt.xlabel(\"Number of latent variables\")\n",
        "plt.ylabel(\"PRESS score\")\n",
        "plt.legend(loc=\"upper left\")\n",
        "if figs_dir: plt.savefig(os.path.join(figs_dir, \"dynamic_models_CV_top10_PRESS.png\"))\n",
        "plt.show()\n",
        "\n",
        "# Q2 over number of components for 10 models which achieved the highest Q2 score \n",
        "plt.figure()\n",
        "for max_lag, win_size in summary.nlargest(10, columns='Q2').index: \n",
        "    model_results = results[(results['number of lags']==max_lag) & (results['window size']==win_size)]\n",
        "    model_results = model_results.sort_values(\"number of components\")\n",
        "    plt.plot(model_results['number of components'], model_results['Q2'], label=f\"lags {max_lag}, wsize {win_size}\")\n",
        "plt.grid()\n",
        "plt.title(\"Q2 over number of latent variables; 10 best models\")\n",
        "plt.xlabel(\"Number of latent variables\")\n",
        "plt.ylabel(\"Q2 score\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "if figs_dir: plt.savefig(os.path.join(figs_dir, \"dynamic_models_CV_top10_Q2.png\"))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "179069ec",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate summary statistics: models grouped by maximum lag and window size, min PRESS scores and max Q2 scores achieved\n",
        "summary = results.groupby(['number of lags', 'window size'], as_index=False).agg(PRESS=('PRESS', 'min'), Q2=('Q2', 'max'))\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# PRESS scores as a heatmap\n",
        "pivot_press = summary.pivot(index='number of lags', columns='window size', values='PRESS')\n",
        "sns.heatmap(pivot_press.astype(float), cmap=\"crest\", ax=axes[0],\n",
        "    xticklabels=pivot_press.columns,\n",
        "    yticklabels=pivot_press.index,\n",
        ")\n",
        "axes[0].set(xlabel='Window size', ylabel='Number of lags')\n",
        "axes[0].set_title(f\"Lowest PRESS scores as a heatmap\\nBrighter is better\")\n",
        "\n",
        "# Q2 scores as a heatmap\n",
        "pivot_Q2 = summary.pivot(index='number of lags', columns='window size', values='Q2')\n",
        "sns.heatmap(pivot_Q2.astype(float), cmap=\"crest_r\", ax=axes[1],\n",
        "    xticklabels=pivot_Q2.columns,\n",
        "    yticklabels=pivot_Q2.index,\n",
        ")\n",
        "axes[1].set(xlabel='Window size', ylabel='Number of lags')\n",
        "axes[1].set_title(f\"Highest Q2 scores as a heatmap\\nBrighter is better\")\n",
        "\n",
        "# Configure layout, save figure as image\n",
        "plt.tight_layout()\n",
        "if figs_dir: plt.savefig(os.path.join(figs_dir, f\"dynamic_models_PRESS_Q2_heatmaps.png\"), dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e978d20f",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dda85879",
      "metadata": {},
      "source": [
        "## Dynamic model validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c3ef075",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Choose best result according to Q2\n",
        "best_result_PRESS = results.iloc[results['PRESS'].argmin()]\n",
        "best_result_Q2 = results.iloc[results['Q2'].argmax()]\n",
        "best_result = best_result_Q2\n",
        "final_params = {\n",
        "    'num_components': int(best_result['number of components']), \n",
        "    'max_lag': int(best_result['number of lags']), \n",
        "    'window_size': int(best_result['window size'])\n",
        "}\n",
        "print(final_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f8b795c",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb10389f",
      "metadata": {},
      "source": [
        "## Dynamic model testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbbf21a0",
      "metadata": {},
      "outputs": [],
      "source": [
        "final_model = DynamicPLSModel(num_components=final_params['num_components'], window_size=final_params['window_size'])\n",
        "\n",
        "selected_lags = np.arange(0, final_params['max_lag']+1)\n",
        "X_test_lags, y_test_lags = make_lagged(X_test, y_test, selected_lags)\n",
        "X_test_lags = X_test_lags.drop(columns=['% Iron Concentrate'])\n",
        "print(X_test_lags.columns)\n",
        "\n",
        "y_pred_test, y_true_test = final_model.rolling_predict(X_test_lags, y_test_lags)\n",
        "\n",
        "if final_model.window_size >= len(X_test_lags): print(f\"Using window size {len(X_test_lags) - 1} (since length of X_test is {len(X_test_lags)})\")\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(np.arange(len(y_pred_test)), y_true_test, label=\"true\")\n",
        "plt.plot(np.arange(len(y_pred_test)), y_pred_test, label=\"predicted\")\n",
        "plt.grid()\n",
        "plt.title(\"Model prediction on test set\")\n",
        "plt.xlabel(\"Observation index\")\n",
        "plt.ylabel(\"% Silica Concentrate\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fa04195",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98d981c8",
      "metadata": {},
      "source": [
        "## Sources\n",
        "\n",
        " \n",
        "- GeeksForGeeks. Rolling regression. Available: https://www.geeksforgeeks.org/machine-learning/rolling-regression/ "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
